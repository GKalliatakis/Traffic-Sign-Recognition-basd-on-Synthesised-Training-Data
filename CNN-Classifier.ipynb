{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/alexandrosstergiou/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "from skimage import color, exposure, transform\n",
    "from PIL import Image \n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from skimage import data, img_as_float\n",
    "from skimage import exposure\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Convolution2D \n",
    "from keras.layers import Input, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from keras.utils import np_utils \n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def analyse_images(imgs_paths):\n",
    "    \n",
    "    data = []\n",
    "    i = 0\n",
    "\n",
    "    for img_path in imgs_paths:\n",
    "        img_class = int(img_path.split('/')[-2])\n",
    "        \n",
    "        img = io.imread(img_path)\n",
    "        \n",
    "        \"\"\"\"\n",
    "        print \"ORIGINAL IMAGE:\"\n",
    "        plt.imshow(img,aspect=\"auto\")\n",
    "        plt.show()\n",
    "        print '\\n'\n",
    "        \"\"\"\n",
    "\n",
    "        rows,col = (img.shape[:2])\n",
    "        if rows>col:\n",
    "            new_side = col\n",
    "        else:\n",
    "            new_side = rows\n",
    "        \n",
    "        srows =  rows//2-new_side//2\n",
    "        frows =  rows//2+new_side//2\n",
    "        \n",
    "        scol =  col//2-new_side//2\n",
    "        fcol =  col//2+new_side//2\n",
    "        \n",
    "        img = img[srows:frows,scol:fcol,:]\n",
    "\n",
    "        # rescale to standard size\n",
    "        img = transform.resize(img, (48, 48))\n",
    "        \n",
    "        # Contrast stretching\n",
    "        p2, p98 = np.percentile(img, (2, 98))\n",
    "        img_contrast = exposure.rescale_intensity(img, in_range=(p2, p98))\n",
    "\n",
    "\n",
    "        # Histogram Equalization\n",
    "        img_eq = exposure.equalize_hist(img)\n",
    "\n",
    "        # Adaptive Equalization\n",
    "        img_adapteq = exposure.equalize_adapthist(img, clip_limit=0.01)\n",
    "\n",
    "        \"\"\"\n",
    "        print \"RESCALED IMAGE:\"\n",
    "        plt.imshow(img,aspect=\"auto\")\n",
    "        plt.show()\n",
    "        print '\\n'\n",
    "        \n",
    "        print \"CONTRAST STRETCHING IMAGE:\"\n",
    "        plt.imshow(img_contrast,aspect=\"auto\")\n",
    "        plt.show()\n",
    "        print '\\n'\n",
    "        \n",
    "        print \"HISTOGRAM EQUATION IMAGE:\"\n",
    "        plt.imshow(img_eq,aspect=\"auto\")\n",
    "        plt.show()\n",
    "        print '\\n'\n",
    "        \n",
    "        print \"ADAPTIVE EQUATION IMAGE:\"\n",
    "        plt.imshow(img_adapteq,aspect=\"auto\")\n",
    "        plt.show()\n",
    "        print '\\n'\n",
    "        \"\"\"\n",
    "        \n",
    "        data.append([img_eq,img_class])\n",
    "        \n",
    "        i = i+1\n",
    "        \n",
    "        if ((i % 1000)== 0):\n",
    "            print (\"Images processed: \")+str(i)\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandrosstergiou/anaconda/lib/python2.7/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/Users/alexandrosstergiou/anaconda/lib/python2.7/site-packages/skimage/exposure/exposure.py:63: UserWarning: This might be a color image. The histogram will be computed on the flattened image. You can instead apply this function to each color channel.\n",
      "  warn(\"This might be a color image. The histogram will be \"\n",
      "/Users/alexandrosstergiou/anaconda/lib/python2.7/site-packages/skimage/util/dtype.py:122: UserWarning: Possible precision loss when converting from float64 to uint16\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images processed: 1000\n",
      "Images processed: 2000\n",
      "Images processed: 3000\n",
      "Images processed: 4000\n",
      "Images processed: 5000\n",
      "Images processed: 6000\n",
      "Images processed: 7000\n",
      "Images processed: 8000\n",
      "Images processed: 9000\n",
      "Images processed: 10000\n",
      "Images processed: 11000\n",
      "Images processed: 12000\n",
      "Images processed: 13000\n",
      "Images processed: 14000\n",
      "Images processed: 15000\n",
      "Images processed: 16000\n",
      "Images processed: 17000\n",
      "Images processed: 18000\n",
      "Images processed: 19000\n",
      "Images processed: 20000\n",
      "Images processed: 21000\n",
      "Images processed: 22000\n",
      "Images processed: 23000\n",
      "Images processed: 24000\n",
      "Images processed: 25000\n",
      "Images processed: 26000\n",
      "Images processed: 27000\n",
      "Images processed: 28000\n",
      "Images processed: 29000\n",
      "Images processed: 30000\n",
      "Images processed: 31000\n",
      "Images processed: 32000\n",
      "Images processed: 33000\n",
      "Images processed: 34000\n",
      "Images processed: 35000\n",
      "Images processed: 36000\n",
      "Images processed: 37000\n",
      "Images processed: 38000\n",
      "Images processed: 39000\n"
     ]
    }
   ],
   "source": [
    "directory = 'GTSRB/Final_Training/Images/'\n",
    "imgs = []\n",
    "labels = []\n",
    "paths = []\n",
    "number_signs = 0\n",
    "\n",
    "for sub_dir in os.listdir(directory):\n",
    "    sd = directory+sub_dir+'/'\n",
    "    if (sub_dir != \".DS_Store\"):\n",
    "        number_signs = number_signs+1\n",
    "        for files in os.listdir(sd):\n",
    "            if (files.endswith(\".csv\")==False):\n",
    "                paths.append(directory+sub_dir+'/'+files)\n",
    "\n",
    "\n",
    "\n",
    "np.random.shuffle(paths)\n",
    "print len(paths)\n",
    "data = analyse_images(paths)\n",
    "elements = []\n",
    "classes = []\n",
    "for d in data:\n",
    "    elements.append(d[0])\n",
    "    classes.append(d[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model(height,width,depth,number_signs):\n",
    "    \n",
    "    \n",
    "    inp = Input(shape=(height, width, depth))\n",
    "    \n",
    "    act= keras.layers.advanced_activations.LeakyReLU(alpha=0.001)\n",
    "    conv_1 = Convolution2D(32, (3, 3), padding='same', activation=act)(inp)\n",
    "    conv_2 = Convolution2D(32, (3, 3), padding='same', activation=act)(conv_1)\n",
    "    \n",
    "    pool_1 = MaxPooling2D((2, 2), data_format=\"channels_last\")(conv_2)\n",
    "    drop_1 = Dropout(0.25)(pool_1)\n",
    "\n",
    "    \n",
    "    conv_3 = Convolution2D(64, (3, 3), padding='same', activation=act)(drop_1)\n",
    "    conv_4 = Convolution2D(64, (3, 3), padding='same', activation=act)(conv_3)\n",
    "\n",
    "    pool_2 = MaxPooling2D((2, 2), data_format=\"channels_last\")(conv_4)\n",
    "    drop_2 = Dropout(0.25)(pool_2)\n",
    "    \n",
    "    conv_5 = Convolution2D(128, (3, 3), padding='same', activation=act)(drop_2)\n",
    "    conv_6 = Convolution2D(128, (3, 3), padding='same', activation=act)(conv_5)\n",
    "\n",
    "    pool_3 = MaxPooling2D((2, 2), data_format=\"channels_last\")(conv_6)\n",
    "    drop_3 = Dropout(0.25)(pool_3)\n",
    "    \n",
    "    \n",
    "    act = keras.layers.advanced_activations.ELU(alpha=1.0)\n",
    "\n",
    "    flat = Flatten()(drop_3)\n",
    "    hidden = Dense(256, activation='relu')(flat)\n",
    "    drop_4 = Dropout(0.5)(hidden)\n",
    "    out = Dense(number_signs, activation='softmax')(drop_4)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandrosstergiou/anaconda/lib/python2.7/site-packages/keras/activations.py:89: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  ).format(identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 48, 48, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 48, 48, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               1179904   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 43)                11051     \n",
      "=================================================================\n",
      "Total params: 1,477,963\n",
      "Trainable params: 1,477,963\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = cnn_model(48,48,3,number_signs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35288 samples, validate on 3921 samples\n",
      "Epoch 1/30\n",
      "35288/35288 [==============================] - 476s - loss: 0.3158 - acc: 0.9010 - val_loss: 0.0820 - val_acc: 0.9770\n",
      "Epoch 2/30\n",
      "35288/35288 [==============================] - 479s - loss: 0.1482 - acc: 0.9538 - val_loss: 0.0371 - val_acc: 0.9898\n",
      "Epoch 3/30\n",
      "35288/35288 [==============================] - 481s - loss: 0.0976 - acc: 0.9695 - val_loss: 0.0250 - val_acc: 0.9944\n",
      "Epoch 4/30\n",
      "35288/35288 [==============================] - 481s - loss: 0.0879 - acc: 0.9746 - val_loss: 0.0545 - val_acc: 0.9839\n",
      "Epoch 5/30\n",
      "35288/35288 [==============================] - 483s - loss: 0.0715 - acc: 0.9793 - val_loss: 0.0212 - val_acc: 0.9926\n",
      "Epoch 6/30\n",
      "35288/35288 [==============================] - 481s - loss: 0.0660 - acc: 0.9799 - val_loss: 0.0366 - val_acc: 0.9906\n",
      "Epoch 7/30\n",
      "35288/35288 [==============================] - 480s - loss: 0.0542 - acc: 0.9845 - val_loss: 0.0181 - val_acc: 0.9952\n",
      "Epoch 8/30\n",
      "35288/35288 [==============================] - 477s - loss: 0.0609 - acc: 0.9820 - val_loss: 0.0220 - val_acc: 0.9952\n",
      "Epoch 9/30\n",
      "35288/35288 [==============================] - 478s - loss: 0.0572 - acc: 0.9842 - val_loss: 0.0177 - val_acc: 0.9954\n",
      "Epoch 10/30\n",
      "35288/35288 [==============================] - 478s - loss: 0.0503 - acc: 0.9857 - val_loss: 0.0182 - val_acc: 0.9957\n",
      "Epoch 11/30\n",
      "35288/35288 [==============================] - 479s - loss: 0.0593 - acc: 0.9830 - val_loss: 0.0129 - val_acc: 0.9959\n",
      "Epoch 12/30\n",
      "35288/35288 [==============================] - 478s - loss: 0.0539 - acc: 0.9846 - val_loss: 0.0319 - val_acc: 0.9921\n",
      "Epoch 13/30\n",
      "35288/35288 [==============================] - 486s - loss: 0.0526 - acc: 0.9847 - val_loss: 0.0173 - val_acc: 0.9952\n",
      "Epoch 14/30\n",
      "35288/35288 [==============================] - 487s - loss: 0.0506 - acc: 0.9867 - val_loss: 0.0167 - val_acc: 0.9962\n",
      "Epoch 15/30\n",
      "35288/35288 [==============================] - 503s - loss: 0.0516 - acc: 0.9868 - val_loss: 0.0273 - val_acc: 0.9946\n",
      "Epoch 16/30\n",
      "35288/35288 [==============================] - 503s - loss: 0.0549 - acc: 0.9851 - val_loss: 0.0155 - val_acc: 0.9954\n",
      "Epoch 17/30\n",
      "35288/35288 [==============================] - 509s - loss: 0.0491 - acc: 0.9865 - val_loss: 0.0170 - val_acc: 0.9954\n",
      "Epoch 18/30\n",
      "35288/35288 [==============================] - 520s - loss: 0.0474 - acc: 0.9878 - val_loss: 0.0582 - val_acc: 0.9918\n",
      "Epoch 19/30\n",
      "35288/35288 [==============================] - 523s - loss: 0.0587 - acc: 0.9859 - val_loss: 0.0132 - val_acc: 0.9974\n",
      "Epoch 20/30\n",
      "35288/35288 [==============================] - 484s - loss: 0.0521 - acc: 0.9873 - val_loss: 0.0177 - val_acc: 0.9957\n",
      "Epoch 21/30\n",
      "35288/35288 [==============================] - 477s - loss: 0.0555 - acc: 0.9863 - val_loss: 0.0167 - val_acc: 0.9957\n",
      "Epoch 22/30\n",
      "35288/35288 [==============================] - 488s - loss: 0.0609 - acc: 0.9862 - val_loss: 0.0201 - val_acc: 0.9967\n",
      "Epoch 23/30\n",
      "35288/35288 [==============================] - 480s - loss: 0.0415 - acc: 0.9898 - val_loss: 0.0303 - val_acc: 0.9941\n",
      "Epoch 24/30\n",
      "35288/35288 [==============================] - 481s - loss: 0.0706 - acc: 0.9854 - val_loss: 0.0229 - val_acc: 0.9954\n",
      "Epoch 25/30\n",
      "35288/35288 [==============================] - 482s - loss: 0.0600 - acc: 0.9865 - val_loss: 0.0180 - val_acc: 0.9959\n",
      "Epoch 26/30\n",
      "35288/35288 [==============================] - 482s - loss: 0.0524 - acc: 0.9881 - val_loss: 0.0129 - val_acc: 0.9972\n",
      "Epoch 27/30\n",
      "35288/35288 [==============================] - 491s - loss: 0.0664 - acc: 0.9857 - val_loss: 0.0250 - val_acc: 0.9946\n",
      "Epoch 28/30\n",
      "35288/35288 [==============================] - 510s - loss: 0.0776 - acc: 0.9847 - val_loss: 0.0271 - val_acc: 0.9936\n",
      "Epoch 29/30\n",
      "35288/35288 [==============================] - 533s - loss: 0.0561 - acc: 0.9876 - val_loss: 0.0923 - val_acc: 0.9811\n",
      "Epoch 30/30\n",
      "35288/35288 [==============================] - 502s - loss: 0.0637 - acc: 0.9867 - val_loss: 0.0116 - val_acc: 0.9985\n",
      "('Test score:', 0.011607913248890961)\n",
      "('Test accuracy:', 0.99846977811782711)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "nb_epochs = 30\n",
    "\n",
    "\n",
    "X = np.array(elements, dtype='float32')\n",
    "Y = np.eye(number_signs, dtype='uint8')[classes]\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y,test_size=0.1, random_state=42)\n",
    "\n",
    "model.fit(X_train, Y_train,batch_size=batch_size,epochs=nb_epochs, validation_data=(X_val,Y_val))\n",
    "\n",
    "score = model.evaluate(X_val, Y_val, verbose=0)\n",
    "\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
