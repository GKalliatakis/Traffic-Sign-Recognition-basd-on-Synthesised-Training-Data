{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "import cv2\n",
    "from skimage import io\n",
    "from skimage import color, exposure, transform\n",
    "from PIL import Image, ImageChops, ImageDraw, ImageOps, ImageFilter, ImageStat, ImageEnhance \n",
    "import imutils\n",
    "import argparse\n",
    "import ntpath\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import math\n",
    "from blend_modes import blend_modes\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_paths(directory):\n",
    "    paths = []\n",
    "    for files in os.listdir(directory):\n",
    "        if (files != \".DS_Store\"):\n",
    "            paths.append(directory+'/'+files)\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_borders(img,pix):\n",
    "    borders = []    \n",
    "    for y in range(0,img.size[1]):\n",
    "        found = False\n",
    "        has_grey = False\n",
    "        \n",
    "        for x in range(1,img.size[0]):\n",
    "\n",
    "            r = pix[x,y][0]\n",
    "            g = pix[x,y][1]\n",
    "            b = pix[x,y][2]\n",
    "                \n",
    "            prev_r = pix[x-1,y][0]\n",
    "            prev_g = pix[x-1,y][1]\n",
    "            prev_b = pix[x-1,y][2]\n",
    "            \n",
    "            r_g = abs(r-g)\n",
    "            r_b = abs(r-b)\n",
    "            g_b = abs(g-b)\n",
    "            \n",
    "            if (r_g<=15 and r_b<=15 and g_b<=15):\n",
    "                has_grey = True\n",
    "\n",
    "                \n",
    "            if ((abs(prev_r-r)>=20) or (abs(prev_g-g)>=20) or (abs(prev_b-b)>=20)):\n",
    "                    \n",
    "                for i in range(0,x):\n",
    "                    borders.append([i,y])\n",
    "                found = True\n",
    "                break   \n",
    "            \n",
    "        if ((not found)and(has_grey)):\n",
    "            for i in range(0,img.size[0]-1):\n",
    "                borders.append([i,y])  \n",
    "              \n",
    "        for x in range(img.size[0]-1,1,-1):\n",
    "                \n",
    "            r = pix[x,y][0]\n",
    "            g = pix[x,y][1]\n",
    "            b = pix[x,y][2]\n",
    "                \n",
    "            prev_r = pix[x-1,y][0]\n",
    "            prev_g = pix[x-1,y][1]\n",
    "            prev_b = pix[x-1,y][2]\n",
    "                \n",
    "            if ((abs(prev_r-r)>=20) or (abs(prev_g-g)>=20) or (abs(prev_b-b)>=20)):\n",
    "                    \n",
    "                for i in range(x,img.size[0]-1):\n",
    "                    borders.append([i,y])\n",
    "                break\n",
    "                    \n",
    "    return borders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def manipulate_images(paths):\n",
    "    \n",
    "    for image_path in paths:\n",
    "        \n",
    "        \n",
    "        \n",
    "        img = Image.open(image_path)\n",
    "        pix = img.load()\n",
    "        \n",
    "        borders = find_borders(img,pix)\n",
    "         \n",
    "        \n",
    "        image = cv2.imread(image_path, -1)\n",
    "        \n",
    "        b_channel, g_channel, r_channel = cv2.split(image)\n",
    "        alpha_channel = np.ones(b_channel.shape, dtype=b_channel.dtype) * 255\n",
    "        image_RGBA = cv2.merge((b_channel, g_channel, r_channel, alpha_channel))\n",
    "        \n",
    "        height, width, channels = image.shape\n",
    "        \n",
    "        #Deleting white perimeter of shape\n",
    "        for i in range(0,img.size[0]-1):\n",
    "            image_RGBA[0,i][3] = 0\n",
    "            image_RGBA[img.size[1]-1,i][3] = 0\n",
    "        \n",
    "        for i in range(0,img.size[1]-1):\n",
    "            image_RGBA[i,0][3] = 0\n",
    "            image_RGBA[i,img.size[0]-1][3] = 0\n",
    "            \n",
    "        \n",
    "        \n",
    "        for border in borders:\n",
    "            image_RGBA[border[1],border[0]][3] = 0\n",
    "            \n",
    "        \n",
    "        head, tail = ntpath.split(image_path)\n",
    "        \n",
    "        title,extension = tail.split('.')\n",
    "        cv2.imwrite(\"Traffic_Signs_Templates/Processed_Images/\"+title+\".png\", image_RGBA)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "directory = 'Traffic_Signs_Templates/Images'\n",
    "if (not os.path.exists(\"Traffic_Signs_Templates/Processed_Images\")):\n",
    "    os.mkdir(\"Traffic_Signs_Templates/Processed_Images\")\n",
    "paths = load_paths(directory)\n",
    "manipulate_images(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def img_transform(paths):\n",
    "    \n",
    "    for image_path in paths:\n",
    "        \n",
    "        img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "        rows,cols,ch = img.shape\n",
    "        t = []\n",
    "        for i in range(1,100):\n",
    "            t.append(i)\n",
    "            \n",
    "        \n",
    "        \n",
    "        #EAST FACING\n",
    "        pts1 = np.float32([[cols/10,rows/10],[cols/2,rows/10],[cols/10,rows/2]])\n",
    "        pts2 = np.float32([[cols/5,rows/5],[cols/2,rows/8],[cols/5,rows/1.8]])\n",
    "        M = cv2.getAffineTransform(pts1,pts2)\n",
    "        dst = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #NORTH-WEST FACING\n",
    "        pts3 = np.float32([[cols*9/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts4 = np.float32([[cols*4.5/5,rows/5],[cols/2,rows/8],[cols*4.5/5,rows/1.8]])\n",
    "        M = cv2.getAffineTransform(pts3,pts4)\n",
    "        dst2 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #LEFT TILTED FORWARD FACING\n",
    "        pts5 = np.float32([[cols/10,rows/10],[cols/2,rows/10],[cols/10,rows/2]])\n",
    "        pts6 = np.float32([[cols/12,rows/6],[cols/2.1,rows/8],[cols/10,rows/1.8]])\n",
    "        M = cv2.getAffineTransform(pts5,pts6)\n",
    "        dst3 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #RIGHT TILTED FORWARD FACING\n",
    "        pts7 = np.float32([[cols*9/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts8 = np.float32([[cols*10/12,rows/6],[cols/2.2,rows/8],[cols*8.4/10,rows/1.8]])\n",
    "        M = cv2.getAffineTransform(pts7,pts8)\n",
    "        dst4 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #WEST FACING\n",
    "        pts9 = np.float32([[cols/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts10 = np.float32([[cols/6,rows/7],[cols/2.2,rows/6],[cols*7/10,rows/1.8]])\n",
    "        M = cv2.getAffineTransform(pts9,pts10)\n",
    "        dst5 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #RIGHT TILTED FORWARD FACING\n",
    "        pts11 = np.float32([[cols*9/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts12 = np.float32([[cols*4/6,rows/6],[cols/2.8,rows/8],[cols*7/10,rows/1.8]])\n",
    "        M = cv2.getAffineTransform(pts11,pts12)\n",
    "        dst6 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #FORWARD FACING W/ DISTORTION\n",
    "        pts13 = np.float32([[cols/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts14 = np.float32([[cols/8,rows/6],[cols/2.5,rows/6.5],[cols*7/10,rows/1.95]])\n",
    "        M = cv2.getAffineTransform(pts13,pts14)\n",
    "        dst7 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #FORWARD FACING W/ DISTORTION 2\n",
    "        pts15 = np.float32([[cols*9/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts16 = np.float32([[cols*4/6,rows/6],[cols/2.5,rows/6.5],[cols*7/10,rows/1.95]])\n",
    "        M = cv2.getAffineTransform(pts15,pts16)\n",
    "        dst8 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #FORWARD FACING W/ DISTORTION 3\n",
    "        pts17 = np.float32([[cols*9/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts18 = np.float32([[cols*4.3/6,rows/6.5],[cols/2.4,rows/6.5],[cols*7/10,rows/1.95]])\n",
    "        M = cv2.getAffineTransform(pts17,pts18)\n",
    "        dst9 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #FORWARD FACING W/ DISTORTION 4\n",
    "        pts19 = np.float32([[cols*9/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts20 = np.float32([[cols*4.1/6,rows/6.5],[cols/2.65,rows/6.6],[cols*7.1/10,rows/2]])\n",
    "        M = cv2.getAffineTransform(pts19,pts20)\n",
    "        dst10 = cv2.warpAffine(img,M,(cols,rows))\n",
    "         \n",
    "        \n",
    "        #plt.imshow(dst10)\n",
    "        \n",
    "        \n",
    "        head, tail = ntpath.split(image_path)\n",
    "        \n",
    "        title,extension = tail.split('.')\n",
    "        \n",
    "        \n",
    "        cv2.imwrite(\"Traffic_Signs_Templates/Transformed_Images/\"+title+\"/\"+str(t[0])+\".png\",dst)\n",
    "        cv2.imwrite(\"Traffic_Signs_Templates/Transformed_Images/\"+title+\"/\"+str(t[1])+\".png\",dst2)\n",
    "        cv2.imwrite(\"Traffic_Signs_Templates/Transformed_Images/\"+title+\"/\"+str(t[2])+\".png\",dst3)\n",
    "        cv2.imwrite(\"Traffic_Signs_Templates/Transformed_Images/\"+title+\"/\"+str(t[3])+\".png\",dst4)\n",
    "        cv2.imwrite(\"Traffic_Signs_Templates/Transformed_Images/\"+title+\"/\"+str(t[4])+\".png\",dst5)\n",
    "        cv2.imwrite(\"Traffic_Signs_Templates/Transformed_Images/\"+title+\"/\"+str(t[5])+\".png\",dst6)\n",
    "        cv2.imwrite(\"Traffic_Signs_Templates/Transformed_Images/\"+title+\"/\"+str(t[6])+\".png\",dst7)\n",
    "        cv2.imwrite(\"Traffic_Signs_Templates/Transformed_Images/\"+title+\"/\"+str(t[7])+\".png\",dst8)\n",
    "        cv2.imwrite(\"Traffic_Signs_Templates/Transformed_Images/\"+title+\"/\"+str(t[8])+\".png\",dst9)\n",
    "        cv2.imwrite(\"Traffic_Signs_Templates/Transformed_Images/\"+title+\"/\"+str(t[9])+\".png\",dst10)\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directory = 'Traffic_Signs_Templates/Processed_Images'\n",
    "if (not os.path.exists(\"Traffic_Signs_Templates/Transformed_Images\")):\n",
    "    for path in paths:\n",
    "        head, tail = ntpath.split(path)    \n",
    "        title,extension = tail.split('.')\n",
    "        os.makedirs(\"Traffic_Signs_Templates/Transformed_Images/\"+title)\n",
    "paths = load_paths(directory)\n",
    "img_transform(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_image_exposure(paths,channels):\n",
    "    \n",
    "    exposures = []\n",
    "    \n",
    "    for image_path in paths:\n",
    "        img = Image.open(image_path)\n",
    "        im = Image.open(image_path).convert('LA')\n",
    "        \n",
    "        stat = ImageStat.Stat(im)\n",
    "        \n",
    "        #Average pixel brighness\n",
    "        avg = stat.mean[0]\n",
    "        \n",
    "        #RMS pixel brighness\n",
    "        rms = stat.rms[0]\n",
    "        \n",
    "        stat2 = ImageStat.Stat(img)\n",
    "        \n",
    "        #Consider the number of channels\n",
    "        #background may have RGB while traffic sign has RGBA\n",
    "        if (channels==3):\n",
    "            #Average pixels preceived brightness\n",
    "            r,g,b = stat2.mean\n",
    "            avg_perceived = math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2))\n",
    "\n",
    "            #RMS pixels perceived brightness\n",
    "            r,g,b = stat2.rms\n",
    "            rms_perceived = math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2)) \n",
    "\n",
    "            l = [image_path,avg,rms,avg_perceived,rms_perceived]\n",
    "            exposures.append(l)\n",
    "        else:\n",
    "            #Average pixels preceived brightness\n",
    "            r,g,b,a = stat2.mean\n",
    "            avg_perceived = math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2))\n",
    "\n",
    "            #RMS pixels perceived brightness\n",
    "            r,g,b,a = stat2.rms\n",
    "            rms_perceived = math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2)) \n",
    "\n",
    "            l = [image_path,avg,rms,avg_perceived,rms_perceived]\n",
    "            exposures.append(l)\n",
    "             \n",
    "\n",
    "    return exposures     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_png(directory):\n",
    "    for files in load_paths(directory):\n",
    "        title,extension = files.split('.')\n",
    "        img = Image.open(files).convert('RGBA')\n",
    "        if (not extension == \"png\"):\n",
    "            os.remove(files)\n",
    "        img.save(title+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_png(\"Google_search_backgrounds/UK_urban\")\n",
    "to_png(\"Google_search_backgrounds/UK_rural\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exposure_manipulation(signs_paths,backgrounds_paths):\n",
    "    \n",
    "    background_exposures = find_image_exposure(background_paths,4)\n",
    "    signs_exposures = find_image_exposure(signs_paths,4)\n",
    "    \n",
    "    for i in range(0,len(background_paths)):\n",
    "        \n",
    "        print \"Processed: \"+str(float(i)/float(len(background_paths))*100)+\" %\"\n",
    "        \n",
    "        img = Image.open(background_exposures[i][0])\n",
    "\n",
    "\n",
    "        for sign_path in signs_paths:\n",
    "\n",
    "            dirc,sub,el = background_exposures[i][0].split('/')\n",
    "            title,extension = el.split('.')\n",
    "\n",
    "            parent_dir,sub_dir,folder,element = sign_path.split('/')\n",
    "            head,tail = element.split('.')\n",
    "\n",
    "            brightness_avrg = 1.0\n",
    "            brightness_rms = 1.0\n",
    "            brightness_avrg_perceived = 1.0\n",
    "            brightness_rms_perceived = 1.0\n",
    "            brightness_avrg2 = 1.0\n",
    "            brightness_rms2 = 1.0\n",
    "            \n",
    "            # abs(desired_brightness - actual_brightness)/ abs(brightness_float_value) = ratio\n",
    "            avrg_ratio = 11.0159464507\n",
    "\n",
    "            rms_ratio = 8.30320014372\n",
    "\n",
    "            percieved_avrg_ratio = 3.85546373056\n",
    "\n",
    "            percieved_rms_ratio = 35.6344530649\n",
    "\n",
    "            avrg2_ratio = 1.20354549572\n",
    "\n",
    "            rms2_ratio = 40.1209106864\n",
    "\n",
    "            peak = Image.open(sign_path).convert('LA')\n",
    "            peak2 = Image.open(sign_path).convert('RGBA')\n",
    "\n",
    "            stat = ImageStat.Stat(peak)\n",
    "            avrg = stat.mean[0]\n",
    "            rms = stat.rms[0]\n",
    "\n",
    "            #IMAGE MANIPULATION MAIN CODE STARTS\n",
    "\n",
    "            #MINIMISE MARGIN BASED ON AVERAGE FOR TWO CHANNEL BRIGNESS VARIATION\n",
    "            margin = abs(avrg-float(background_exposures[i][1]))\n",
    "            \n",
    "            brightness_avrg = margin/avrg_ratio \n",
    "            \n",
    "            enhancer = ImageEnhance.Brightness(peak2)\n",
    "            avrg_bright = enhancer.enhance(brightness_avrg)\n",
    "            stat = ImageStat.Stat(avrg_bright)\n",
    "            avrg = stat.mean[0]\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "            #MINIMISE MARGIN BASED ON ROOT MEAN SQUARE FOR TWO CHANNEL BRIGNESS VARIATION\n",
    "            margin = abs(rms-float(background_exposures[i][2]))\n",
    "\n",
    "            brightness_rms = margin/rms_ratio \n",
    "            \n",
    "            enhancer = ImageEnhance.Brightness(peak2)\n",
    "            rms_bright = enhancer.enhance(brightness_rms)\n",
    "            stat = ImageStat.Stat(rms_bright)\n",
    "            rms = stat.rms[0]\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #MINIMISE MARGIN BASED ON AVERAGE FOR RGBA (\"PERCEIVED BRIGHNESS\")\n",
    "            #REFERENCE FOR ALGORITHM USED: http://alienryderflex.com/hsp.html\n",
    "            stat2 = ImageStat.Stat(peak2)\n",
    "\n",
    "            r,g,b,a = stat2.mean\n",
    "            avrg_perceived = math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2))\n",
    "            margin = abs(avrg_perceived-float(background_exposures[i][3]))\n",
    "            \n",
    "            brightness_avrg_perceived = margin/percieved_avrg_ratio \n",
    "            \n",
    "            enhancer = ImageEnhance.Brightness(peak2)\n",
    "            avrg_bright_perceived = enhancer.enhance(brightness_avrg_perceived)\n",
    "            stat2 = ImageStat.Stat(avrg_bright_perceived)\n",
    "            r,g,b,a = stat2.mean\n",
    "            avrg_perceived = math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2))        \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            #MINIMISE MARGIN BASED ON RMS FOR RGBA (\"PERCEIVED BRIGHNESS\")\n",
    "            #REFERENCE FOR ALGORITHM USED: http://alienryderflex.com/hsp.html\n",
    "            r,g,b,a = stat2.rms\n",
    "            rms_perceived = math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2))\n",
    "\n",
    "            margin = abs(rms_perceived-float(background_exposures[i][4]))\n",
    "\n",
    "            brightness_rms_perceived = margin/percieved_rms_ratio \n",
    "\n",
    "            enhancer = ImageEnhance.Brightness(peak2)\n",
    "            rms_bright_perceived = enhancer.enhance(brightness_rms_perceived)\n",
    "            stat2 = ImageStat.Stat(rms_bright_perceived)\n",
    "            r,g,b,a = stat2.rms\n",
    "            rms_perceived = math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2))        \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            stat3 = ImageStat.Stat(peak2)\n",
    "            avrg2 = stat3.mean[0]\n",
    "            rms2 = stat3.rms[0]\n",
    "\n",
    "\n",
    "            #FUSION OF THE TWO AVERAGING METHODS\n",
    "            margin = abs(avrg2-float(background_exposures[i][1]))\n",
    "\n",
    "            brightness_avrg2 = margin/avrg2_ratio \n",
    "    \n",
    "            enhancer = ImageEnhance.Brightness(peak2)\n",
    "            avrg_bright2 = enhancer.enhance(brightness_avrg2)\n",
    "            stat3 = ImageStat.Stat(avrg_bright2)\n",
    "            avrg2 = stat3.mean[0]       \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            #FUSION OF THE TWO RMS METHODS\n",
    "            margin = abs(rms2-float(background_exposures[i][2]))\n",
    "            \n",
    "            brightness_rms2 = margin/rms2_ratio \n",
    "            \n",
    "            enhancer = ImageEnhance.Brightness(peak2)\n",
    "            rms_bright2 = enhancer.enhance(brightness_rms2)\n",
    "            stat3 = ImageStat.Stat(rms_bright2)\n",
    "            rms2 = stat3.rms[0]        \n",
    "            \n",
    "            avrg_bright = avrg_bright.resize((150,150), Image.ANTIALIAS)\n",
    "            rms_bright = rms_bright.resize((150,150), Image.ANTIALIAS)\n",
    "            avrg_bright_perceived = avrg_bright_perceived.resize((150,150), Image.ANTIALIAS)\n",
    "            rms_bright_perceived = rms_bright_perceived.resize((150,150), Image.ANTIALIAS)\n",
    "            avrg_bright2 = avrg_bright2.resize((150,150), Image.ANTIALIAS)\n",
    "            rms_bright2 = rms_bright2.resize((150,150), Image.ANTIALIAS)\n",
    "\n",
    "            \n",
    "            avrg_bright.save(\"Traffic_Signs_exposure_manipulation/\"+sub+\"/\"+title+\"/SIGN_\"+folder+\"/\"+head+\"_AVERAGE.\"+tail)\n",
    "            rms_bright.save(\"Traffic_Signs_exposure_manipulation/\"+sub+\"/\"+title+\"/SIGN_\"+folder+\"/\"+head+\"_RMS.\"+tail)\n",
    "            avrg_bright_perceived.save(\"Traffic_Signs_exposure_manipulation/\"+sub+\"/\"+title+\"/SIGN_\"+folder+\"/\"+head+\"_AVERAGE_PERCEIVED.\"+tail)\n",
    "            rms_bright_perceived.save(\"Traffic_Signs_exposure_manipulation/\"+sub+\"/\"+title+\"/SIGN_\"+folder+\"/\"+head+\"_RMS_PERCEIVED.\"+tail)\n",
    "            avrg_bright2.save(\"Traffic_Signs_exposure_manipulation/\"+sub+\"/\"+title+\"/SIGN_\"+folder+\"/\"+head+\"_AVERAGE2.\"+tail)\n",
    "            rms_bright2.save(\"Traffic_Signs_exposure_manipulation/\"+sub+\"/\"+title+\"/SIGN_\"+folder+\"/\"+head+\"_RMS2.\"+tail)\n",
    "    print \"Processed: \"+str(100)+\" %\"\n",
    "    print \"Process was successful\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 0.0 %\n",
      "Processed: 2.0 %\n",
      "Processed: 4.0 %\n",
      "Processed: 6.0 %\n",
      "Processed: 8.0 %\n",
      "Processed: 10.0 %\n",
      "Processed: 12.0 %\n",
      "Processed: 14.0 %\n",
      "Processed: 16.0 %\n",
      "Processed: 18.0 %\n",
      "Processed: 20.0 %\n",
      "Processed: 22.0 %\n",
      "Processed: 24.0 %\n",
      "Processed: 26.0 %\n",
      "Processed: 28.0 %\n",
      "Processed: 30.0 %\n",
      "Processed: 32.0 %\n",
      "Processed: 34.0 %\n",
      "Processed: 36.0 %\n",
      "Processed: 38.0 %\n",
      "Processed: 40.0 %\n",
      "Processed: 42.0 %\n",
      "Processed: 44.0 %\n",
      "Processed: 46.0 %\n",
      "Processed: 48.0 %\n",
      "Processed: 50.0 %\n",
      "Processed: 52.0 %\n",
      "Processed: 54.0 %\n",
      "Processed: 56.0 %\n",
      "Processed: 58.0 %\n",
      "Processed: 60.0 %\n",
      "Processed: 62.0 %\n",
      "Processed: 64.0 %\n",
      "Processed: 66.0 %\n",
      "Processed: 68.0 %\n",
      "Processed: 70.0 %\n",
      "Processed: 72.0 %\n",
      "Processed: 74.0 %\n",
      "Processed: 76.0 %\n",
      "Processed: 78.0 %\n",
      "Processed: 80.0 %\n",
      "Processed: 82.0 %\n",
      "Processed: 84.0 %\n",
      "Processed: 86.0 %\n",
      "Processed: 88.0 %\n",
      "Processed: 90.0 %\n",
      "Processed: 92.0 %\n",
      "Processed: 94.0 %\n",
      "Processed: 96.0 %\n",
      "Processed: 98.0 %\n",
      "Processed: 100 %\n",
      "Process was successful\n"
     ]
    }
   ],
   "source": [
    "bg_dir = \"Google_search_backgrounds\"\n",
    "\n",
    "for dirs in load_paths(bg_dir):    \n",
    "    initial,subd = dirs.split('/')\n",
    "    \n",
    "    for background in load_paths(dirs):\n",
    "        initial,subd,element = background.split('/')\n",
    "        title,extension = element.split('.')\n",
    "        \n",
    "        for signp in load_paths(\"Traffic_Signs_Templates/Transformed_Images\"):\n",
    "            for sign in load_paths(signp):\n",
    "                d,s,f,e = sign.split('/')\n",
    "                head,tail = e.split('.')\n",
    "            \n",
    "                if (not os.path.exists(\"Traffic_Signs_exposure_manipulation/\"+subd+\"/\"+title+\"/SIGN_\"+f)):\n",
    "                    os.makedirs(\"Traffic_Signs_exposure_manipulation/\"+subd+\"/\"+title+\"/SIGN_\"+f)\n",
    "            \n",
    "\n",
    "signs_paths = []\n",
    "for p in load_paths(\"Traffic_Signs_Templates/Transformed_Images\"):\n",
    "    signs_paths = signs_paths + load_paths(p)\n",
    "\n",
    "background_paths = load_paths(\"Google_search_backgrounds/UK_urban\")\n",
    "exposure_manipulation(signs_paths,background_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 0.0 %\n",
      "Processed: 2.0 %\n",
      "Processed: 4.0 %\n",
      "Processed: 6.0 %\n",
      "Processed: 8.0 %\n",
      "Processed: 10.0 %\n",
      "Processed: 12.0 %\n",
      "Processed: 14.0 %\n",
      "Processed: 16.0 %\n",
      "Processed: 18.0 %\n",
      "Processed: 20.0 %\n",
      "Processed: 22.0 %\n",
      "Processed: 24.0 %\n",
      "Processed: 26.0 %\n",
      "Processed: 28.0 %\n",
      "Processed: 30.0 %\n",
      "Processed: 32.0 %\n",
      "Processed: 34.0 %\n",
      "Processed: 36.0 %\n",
      "Processed: 38.0 %\n",
      "Processed: 40.0 %\n",
      "Processed: 42.0 %\n",
      "Processed: 44.0 %\n",
      "Processed: 46.0 %\n",
      "Processed: 48.0 %\n",
      "Processed: 50.0 %\n",
      "Processed: 52.0 %\n",
      "Processed: 54.0 %\n",
      "Processed: 56.0 %\n",
      "Processed: 58.0 %\n",
      "Processed: 60.0 %\n",
      "Processed: 62.0 %\n",
      "Processed: 64.0 %\n",
      "Processed: 66.0 %\n",
      "Processed: 68.0 %\n",
      "Processed: 70.0 %\n",
      "Processed: 72.0 %\n",
      "Processed: 74.0 %\n",
      "Processed: 76.0 %\n",
      "Processed: 78.0 %\n",
      "Processed: 80.0 %\n",
      "Processed: 82.0 %\n",
      "Processed: 84.0 %\n",
      "Processed: 86.0 %\n",
      "Processed: 88.0 %\n",
      "Processed: 90.0 %\n",
      "Processed: 92.0 %\n",
      "Processed: 94.0 %\n",
      "Processed: 96.0 %\n",
      "Processed: 98.0 %\n",
      "Processed: 100 %\n",
      "Process was successful\n"
     ]
    }
   ],
   "source": [
    "background_paths = load_paths(\"Google_search_backgrounds/UK_rural\")\n",
    "exposure_manipulation(signs_paths,background_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avrg_pixel_rgb(image,chanels):\n",
    "    stat = ImageStat.Stat(image)\n",
    "    if (chanels == 4):\n",
    "        r,g,b,a = stat.rms\n",
    "    else:\n",
    "        r,g,b = stat.rms\n",
    "    \n",
    "    return [r,g,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_bw_images(directory):\n",
    "    images = []\n",
    "    for signs in load_paths(directory):\n",
    "        img = Image.open(signs).convert('RGBA')\n",
    "        rgb = avrg_pixel_rgb(img,4)\n",
    "        rg = abs(rgb[0]-rgb[1])\n",
    "        rb = abs(rgb[0]-rgb[2])\n",
    "        gb = abs(rgb[1]-rgb[2])\n",
    "        \n",
    "        temp = signs.split('/')\n",
    "        head,tail = temp[-1].split('.')\n",
    "                \n",
    "        if (rg<=1 and rb<=1 and gb<=1):\n",
    "            images.append(head)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_useful_signs(directory):\n",
    "    bw_images = find_bw_images(\"Traffic_Signs_Templates/Images\")\n",
    "    for background_dir in load_paths(directory):\n",
    "        \n",
    "        for signs in load_paths(background_dir):\n",
    "            temp = []\n",
    "            for imgs in load_paths(signs):\n",
    "                temp.append(imgs)\n",
    "            exposures = find_image_exposure(temp,4)\n",
    "            i = 0\n",
    "            for images in load_paths(signs):\n",
    "                \n",
    "                \n",
    "                \n",
    "                #Find brightness\n",
    "                img = Image.open(images).convert('RGBA')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                rgb = avrg_pixel_rgb(img,4)\n",
    "                rg = abs(rgb[0]-rgb[1])\n",
    "                rb = abs(rgb[0]-rgb[2])\n",
    "                gb = abs(rgb[1]-rgb[2])\n",
    "                \n",
    "                    \n",
    "                is_bw = False\n",
    "                \n",
    "                for s in bw_images:\n",
    "                    if s in exposures[i][0]:\n",
    "                        is_bw = True\n",
    "                    \n",
    "                if (rg<=20 and rb<=20 and gb<=20 ):\n",
    "                    if (not is_bw):\n",
    "                        os.remove(images)\n",
    "                    elif (rgb[0]<70 and rgb[1]<70 and rgb[2]<70):\n",
    "                        os.remove(images)\n",
    "                    elif (rgb[0]>160 and rgb[1]>160 and rgb[2]>160):\n",
    "                        os.remove(images)\n",
    "                i = i+1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directory= \"Traffic_Signs_exposure_manipulation/UK_urban\"\n",
    "find_useful_signs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directory= \"Traffic_Signs_exposure_manipulation/UK_rural\"\n",
    "find_useful_signs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_data(image_dir,bg_dir):\n",
    "    \n",
    "    \n",
    "    # Import background image\n",
    "    background_img_raw = Image.open(bg_dir).convert('RGBA')  # RGBA image\n",
    "    background_img_raw = background_img_raw.resize((150,150), Image.ANTIALIAS)\n",
    "    background_img = np.array(background_img_raw)  # Inputs to blend_modes need to be numpy arrays.\n",
    "    background_img_float = background_img.astype(float)  # Inputs to blend_modes need to be floats.\n",
    "\n",
    "    # Import foreground image\n",
    "    foreground_img_raw = Image.open(image_dir)  # RGBA image\n",
    "    foreground_img = np.array(foreground_img_raw)  # Inputs to blend_modes need to be numpy arrays.\n",
    "    foreground_img_float = foreground_img.astype(float)  # Inputs to blend_modes need to be floats.\n",
    "\n",
    "    # Blend images\n",
    "    opacity = 1  # The opacity of the foreground that is blended onto the background is 70 %.\n",
    "    blended_img_float = blend_modes.grain_merge(background_img_float, foreground_img_float, opacity)\n",
    "\n",
    "    # Convert blended image back into PIL image\n",
    "    blended_img = np.uint8(blended_img_float)  # Image needs to be converted back to uint8 type for PIL handling.\n",
    "    blended_img_raw = Image.fromarray(blended_img)  # Note that alpha channels are displayed in black by PIL by default.\n",
    "                                                    # This behavior is difficult to change (although possible).\n",
    "                                                    # If you have alpha channels in your images, then you should give\n",
    "                                                    # OpenCV a try.\n",
    "\n",
    "    # Display blended image\n",
    "    foreground_img_raw = foreground_img_raw.resize((149,149), Image.ANTIALIAS)\n",
    "    blended_img_raw.paste(foreground_img_raw, (0, 0), foreground_img_raw)\n",
    "    blended_img_raw = blended_img_raw.resize((48,48), Image.ANTIALIAS)\n",
    "    return blended_img_raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directory = 'SGTSD/Images'\n",
    "if (not os.path.exists(\"SGTSD/Images\")):\n",
    "    for sign in load_paths(\"Traffic_Signs_Templates/Images\"):\n",
    "        head,tail = sign.split('.')\n",
    "        name = []\n",
    "        name = head.split('/')\n",
    "        os.makedirs(\"SGTSD/Images/\"+name[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content = '''\n",
    "-----------------------------------------------\n",
    "|                     -*-                     |\n",
    "|Synthetically Generated Traffic Sign Dataset |\n",
    "|                     -*-                     |\n",
    "-----------------------------------------------\n",
    "\n",
    "This directory contains the training set for\n",
    "The Convolutional Neural Network (CNN)\n",
    "Used in this project\n",
    "\n",
    "However, it can be used for any classifier\n",
    "desired by the person using the code and\n",
    "additionally, it is not limited to a specific\n",
    "traffic sign templates.\n",
    " \n",
    "\n",
    "----------------------------------------------\n",
    "Content\n",
    "----------------------------------------------\n",
    "\n",
    "The number of example is based on the number:\n",
    "->of traffic signs that were used as templates\n",
    "->of the image manipulation processes\n",
    "->of the brighness variations values used\n",
    "->of the blending procedures\n",
    "\n",
    "\n",
    "----------------------------------------------\n",
    "Image format and naming\n",
    "----------------------------------------------\n",
    "The images created are of \"png\" format\n",
    "with RGBA channels\n",
    "\n",
    "   SIGN_X/XYX_XYX.png\n",
    "\n",
    "The initial part (X) is used to distinguish the\n",
    "sign class, while the remaining (XYX_XYX) firstly\n",
    "indicated the sign in the file itself and the\n",
    "example number.\n",
    "\n",
    "\n",
    "----------------------------------------------\n",
    "Additional information\n",
    "----------------------------------------------\n",
    "\n",
    "contact email: \n",
    "    \n",
    "\tasterga@essex.ac.uk\n",
    "\n",
    "\n",
    "----------------------------------------------\n",
    "Alexandros Stergiou\n",
    "\"The Driver's Assistant\"\n",
    "\n",
    "University of Essex,\n",
    "Schoolf of Computer Science and\n",
    "Electronic Engineering,\n",
    "UK\n",
    "----------------------------------------------\n",
    "'''\n",
    "text_file = open(\"SGTSD/Readme_Images.txt\", \"w\")\n",
    "text_file.write(content)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_paths_list(imgs_directory,bg_directory):\n",
    "    directories = []\n",
    "    for places in load_paths(imgs_directory):\n",
    "        for imgs in load_paths(places):\n",
    "            dr = []\n",
    "            dr = imgs.split('/')\n",
    "            bg = bg_directory +'/'+dr[-2]+'/'+dr[-1]+\".png\"\n",
    "            for signs in load_paths(imgs):\n",
    "                for png in load_paths(signs):\n",
    "                    directories.append([png,bg])\n",
    "    return directories\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files to be generated: 210427\n"
     ]
    }
   ],
   "source": [
    "directories = create_paths_list(\"Traffic_Signs_exposure_manipulation\",\"Google_search_backgrounds\")\n",
    "print \"Files to be generated: \"+str(len(directories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_for_sign_x(i,directories):\n",
    "    l = []\n",
    "    for elements in directories:\n",
    "        foreground = elements[0].split('/')\n",
    "        background = elements[1].split('/')\n",
    "        if (foreground[-2] == (\"SIGN_\"+str(i))):\n",
    "            l.append(elements)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_directories = []\n",
    "signs = load_paths('Traffic_Signs_Templates/Images')\n",
    "for i in range(1,len(signs)+1):\n",
    "    final_directories.append(list_for_sign_x(i,directories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 0.0 %\n",
      "Processed: 2.0 %\n",
      "Processed: 4.0 %\n",
      "Processed: 6.0 %\n",
      "Processed: 8.0 %\n",
      "Processed: 10.0 %\n",
      "Processed: 12.0 %\n",
      "Processed: 14.0 %\n",
      "Processed: 16.0 %\n",
      "Processed: 18.0 %\n",
      "Processed: 20.0 %\n",
      "Processed: 22.0 %\n",
      "Processed: 24.0 %\n",
      "Processed: 26.0 %\n",
      "Processed: 28.0 %\n",
      "Processed: 30.0 %\n",
      "Processed: 32.0 %\n",
      "Processed: 34.0 %\n",
      "Processed: 36.0 %\n",
      "Processed: 38.0 %\n",
      "Processed: 40.0 %\n",
      "Processed: 42.0 %\n",
      "Processed: 44.0 %\n",
      "Processed: 46.0 %\n",
      "Processed: 48.0 %\n",
      "Processed: 50.0 %\n",
      "Processed: 52.0 %\n",
      "Processed: 54.0 %\n",
      "Processed: 56.0 %\n",
      "Processed: 58.0 %\n",
      "Processed: 60.0 %\n",
      "Processed: 62.0 %\n",
      "Processed: 64.0 %\n",
      "Processed: 66.0 %\n",
      "Processed: 68.0 %\n",
      "Processed: 70.0 %\n",
      "Processed: 72.0 %\n",
      "Processed: 74.0 %\n",
      "Processed: 76.0 %\n",
      "Processed: 78.0 %\n",
      "Processed: 80.0 %\n",
      "Processed: 82.0 %\n",
      "Processed: 84.0 %\n",
      "Processed: 86.0 %\n",
      "Processed: 88.0 %\n",
      "Processed: 90.0 %\n",
      "Processed: 92.0 %\n",
      "Processed: 94.0 %\n",
      "Processed: 96.0 %\n",
      "Processed: 98.0 %\n",
      "Processed: 100 %\n"
     ]
    }
   ],
   "source": [
    "direct = \"SGTSD/Images\"\n",
    "i = 1\n",
    "for element in final_directories:\n",
    "    print \"Processed: \"+str(float(i-1)/float(len(final_directories))*100)+\" %\"\n",
    "    j = 1\n",
    "    for dirs in element:\n",
    "        image = new_data(dirs[0],dirs[1])\n",
    "        image.save(direct+\"/\"+str(i)+\"/\"+str(i)+\"_\"+str(j)+\".png\")\n",
    "        j = j+1\n",
    "    i = i+1\n",
    "print \"Processed: \"+str(100)+\" %\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"Traffic_Signs_exposure_manipulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shutil.rmtree(\"Traffic_Signs_Templates/Transformed_Images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shutil.rmtree(\"Traffic_Signs_Templates/Processed_Images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = '''\n",
    "-------------------------------------\n",
    "BREAKDOWN OF FILES GENERATED BY CLASS\n",
    "-------------------------------------\n",
    "'''\n",
    "total = 0\n",
    "for i in range (0,len(final_directories)):\n",
    "    s = \"Generated \"+str(len(final_directories[i]))+\" examples for sign class \"+str(i+1)\n",
    "    string = string + '\\n'+s+'\\n'\n",
    "    total = total + len(final_directories[i])\n",
    "string = string + '\\n'+\"TOTAL: \"+str(total)+'\\n'\n",
    "string = string + \"-------------------------------------\"\n",
    "text_file = open(\"SGTSD/generated_images_about.txt\", \"w\")\n",
    "text_file.write(string)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
